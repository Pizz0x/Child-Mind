{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "097227b6",
   "metadata": {},
   "source": [
    "# **Data Cleaning**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661e550d",
   "metadata": {},
   "source": [
    "First I'm going to look at the data we are working with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b4cb6a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "url_train = \"data/train.csv\"\n",
    "df_train = pd.read_csv(url_train)\n",
    "#df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "36d9f7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset2 = df_train.astype('object')\n",
    "# dataset2.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80df10b7",
   "metadata": {},
   "source": [
    "We can see that we have 3 type of data: int, float and string.\n",
    "`string` data have only 4 possible values: the season, so they are good to be one-hot encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eda240a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inst, num_features = df_train.shape\n",
    "\n",
    "for f in range(num_features):\n",
    "    col = df_train.iloc[:, f].astype(str)\n",
    "    #print(f, np.unique(col))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c216f9c5",
   "metadata": {},
   "source": [
    "Another thing we can see is that most of the features have `nan` value, so we also have to deal with missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e612c9f",
   "metadata": {},
   "source": [
    "We can get additional clues by looking at `data_dictionary.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e7c0e011",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"data/data_dictionary.csv\"\n",
    "description = pd.read_csv(url)\n",
    "#description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb0f976",
   "metadata": {},
   "source": [
    "Now that we have additional information about the dataset, we can start the data cleaning process, we will start by removing the rows where the value of `sii` is `NaN` since it's the features that we use for our supervised learning. <br>\n",
    "We will then remove the column that represent the id feature since it's used as \"primary key\" to distinguish the rows and it's not relevant for a classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7986afa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove rows where the value of sii is NaN\n",
    "df_train.dropna(subset=['sii'], inplace=True)\n",
    "\n",
    "# remove the column id\n",
    "del df_train['id'] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cdb01ee",
   "metadata": {},
   "source": [
    "We can now start dealing with the missing values.\n",
    "I've notice that in the dataset we have a group of Physical Measures like \"weight\" and \"height\", that for child are related mostly to the age and the sex of the specific children.<br>\n",
    "Luckily we can see that age and sex are feature that are never null in our dataset, so I thought that we could use them to insert in the rows where the phisical measures are missing the average value for the specific age and sex of the child. <br>\n",
    "I did this only in the rows where all the physical measures are missing, since in the case some of them are missing and some are present, I thought that using a KNN-Imputer was a better idea. <br>\n",
    "To do this I used an external source, since I thought that it would be more reliable than trying to predict the values with the mean or other ways. <br>\n",
    "The external source is a csv file that I've created using the NHANES data that assess the health and nutritional status of children in the United States. <br>\n",
    "I decided to use this approach since it limits the distortion of the measures compared to the global means, indeed a girl of 7 years isn't going to be as tall as a boy of 15 years. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "deef4ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load external file\n",
    "physical_measures_df = pd.read_csv('data/physical_measures.csv')\n",
    "\n",
    "# add the columns of the average physical measures (given an age and a sex) to each row in the dataframe\n",
    "df_train = df_train.merge( physical_measures_df, on=['Basic_Demos-Age', 'Basic_Demos-Sex'], suffixes=('', '_avg'))\n",
    "cols = ['Physical-BMI','Physical-Height','Physical-Weight','Physical-Waist_Circumference','Physical-Diastolic_BP','Physical-HeartRate','Physical-Systolic_BP']\n",
    "# a list of boolean corresponding to each row -> true if the physical measures are all nan\n",
    "tot_nan_phys = df_train[cols].isna().all(axis=1)\n",
    "\n",
    "for col in cols:\n",
    "    # first fill the rows with only nan values for the physical measure with the average\n",
    "    df_train.loc[tot_nan_phys, col] = df_train.loc[tot_nan_phys, f\"{col}_avg\"]\n",
    "    # then remove the average columns\n",
    "    del df_train[f\"{col}_avg\"]\n",
    "    #print(np.unique(df_train[col]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7bab00f",
   "metadata": {},
   "source": [
    "Now that we have enriched our dataset where rows had a very low level of information, we can start working on the rest of the dataset to handle other missing values. <br>\n",
    "What we want to do is :\n",
    "- replace the missing values of the columns with the KNN imputer for the numerical values\n",
    "- replace the missing values of the columns with the mode for the categorical values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b680828",
   "metadata": {},
   "source": [
    "But first it's better to separate the features of the classification task from the one to be classified: `sii`. <br>\n",
    "It's also important to separate the test set from the training set since otherwise the value that are going to substitute the missing values in the test set would be affected by one of the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "28210f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = df_train.iloc[:, :-1]\n",
    "y = df_train.iloc[:, -1]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33322537",
   "metadata": {},
   "source": [
    "We can now start replacing the missing values by dividing the numerical and categorical features and operate on them separately:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3b608a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# array of boolean saying if a specific column is numeric or not\n",
    "is_numerical = np.array([np.issubdtype(dtype, np.number) for dtype in X.dtypes])  \n",
    "numerical_idx = np.flatnonzero(is_numerical) \n",
    "# takes only the column that are numerical\n",
    "new_X_train = X_train.iloc[:, numerical_idx]\n",
    "new_X_test = X_test.iloc[:, numerical_idx]\n",
    "#new_X_train.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c197e78f",
   "metadata": {},
   "source": [
    "Now that we have all the numerical column we can replace the NaN values of specific column with the value of the closer neighbors. <br>\n",
    "To do this I used `KNNImputer`, which fills missing values based on nearest neighbors, in this way we take correlation into account. <br>\n",
    "It's important to scale our values before using the k-nearest neighbors method since otherwise it will consider a lot more field where an high value is a default and not consider the one where a low value is normal. Naturally i will rescale the values to normal once computed the transformation. <br>\n",
    "I decided to use it because in this case it's way better than the mean since, as already mentioned, having child of 6 and 19 years old in the same dataset give us a mean that doesn't represent coherently the specific kid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2d803fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "imputer = KNNImputer(n_neighbors=3)\n",
    "\n",
    "# train set\n",
    "scaled_train = scaler.fit_transform(new_X_train)\n",
    "X_array = imputer.fit_transform(scaled_train)\n",
    "X_array = scaler.inverse_transform(X_array)\n",
    "new_X_train = pd.DataFrame(X_array, columns=new_X_train.columns, index=new_X_train.index) # convert into a dataframe since X_array is of type ndarray\n",
    "\n",
    "# test set\n",
    "scaled_test = scaler.fit_transform(new_X_test)\n",
    "X_array = imputer.fit_transform(scaled_test)\n",
    "X_array = scaler.inverse_transform(X_array)\n",
    "new_X_test = pd.DataFrame(X_array, columns=new_X_test.columns, index=new_X_test.index)\n",
    "\n",
    "#new_X_train.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5797fcf",
   "metadata": {},
   "source": [
    "As we can see in the field where there was a missing value, we substitute it with the total mean of the non-missing values. <br>\n",
    "Let's now check if there are still `NaN` values in the numerical features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "34cf2b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inst, num_features = new_X_train.shape\n",
    "for f in range(num_features):\n",
    "    col = new_X_train.iloc[:, f].astype(str)\n",
    "    #print(f, np.unique(col))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a249551d",
   "metadata": {},
   "source": [
    "We can feel satisfied by this first part of the data processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb09f9a",
   "metadata": {},
   "source": [
    "Now we have to handle the categorical values, we have 2 things to do:\n",
    "- replace missing values with the mode\n",
    "- transform them with One-Hot Encoding <br>\n",
    "\n",
    "I've decided to use the mode to replace missing values because since the number of categories is small (the seasons), we don't need a complex modelling so we can use a simple model.<br>\n",
    "What the mode imputer does is fill the missing values with the most common value of the selected feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0c791ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "categorical_idx = np.flatnonzero(is_numerical==False)\n",
    "categorical_X_train = X_train.iloc[:, categorical_idx]\n",
    "categorical_X_test = X_test.iloc[:, categorical_idx]\n",
    "\n",
    "imputer = SimpleImputer(strategy='most_frequent')\n",
    "X_array = imputer.fit_transform(categorical_X_train)\n",
    "categorical_X_train = pd.DataFrame(X_array, columns=categorical_X_train.columns, index=categorical_X_train.index)\n",
    "\n",
    "X_array = imputer.fit_transform(categorical_X_test)\n",
    "categorical_X_test = pd.DataFrame(X_array, columns=categorical_X_test.columns, index=categorical_X_test.index)\n",
    "\n",
    "#categorical_X_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "25c8eb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now that we have no more missing values, we can handle categorical labels using one-hot encoding\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "oh = OneHotEncoder(sparse_output=False)\n",
    "\n",
    "oh.fit(categorical_X_train)\n",
    "encoded = oh.transform(categorical_X_train)\n",
    "#print(oh.get_feature_names_out())\n",
    "# we now add the encoded string features to the new data frame\n",
    "for i, col in enumerate(oh.get_feature_names_out()):\n",
    "    new_X_train = new_X_train.copy()\n",
    "    new_X_train[col] = encoded[:, i]\n",
    "\n",
    "oh.fit(categorical_X_test)\n",
    "encoded = oh.transform(categorical_X_test)\n",
    "\n",
    "# we now add the encoded string features to the new data frame\n",
    "for i, col in enumerate(oh.get_feature_names_out()):\n",
    "    new_X_test = new_X_test.copy()\n",
    "    new_X_test[col] = encoded[:, i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c496d26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we now a good dataset to train our model with\n",
    "#new_X_train.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb33a0a",
   "metadata": {},
   "source": [
    "Let's now try with a first approach: <br>\n",
    "At first I will calculate the baseline accuracy, that represents the accuracy of a naive classifier that basically classify every instance as if it was of the most frequent in the train set. <br>\n",
    "Then I will use a basic Random Forest model and check its accuracy. <br>\n",
    "Our goal is to at least predict better than the naive classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bb56c725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority class accuracy: 0.589\n",
      "Test Accuracy: 0.989\n"
     ]
    }
   ],
   "source": [
    "baseline_accuracy = y_train.value_counts().max() / y_train.value_counts().sum()\n",
    "print (f\"Majority class accuracy: {baseline_accuracy:.3f}\")\n",
    "# our goal is to have a model that can predict better than the naive classifier\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "model = RandomForestClassifier(max_leaf_nodes=20)\n",
    "model.fit(new_X_train, y_train)\n",
    "\n",
    "test_acc = accuracy_score(y_true = y_test, y_pred = model.predict(new_X_test) )\n",
    "print (\"Test Accuracy: {:.3f}\".format(test_acc) )\n",
    "\n",
    "#print(model.feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335283f0",
   "metadata": {},
   "source": [
    "We can see that even with a basic Random Forest, we get a perfect classifier. <br>\n",
    "But there is a problem, a few features are quite important for the prediction, they are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9a6ff95c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PCIAT-PCIAT_02', 'PCIAT-PCIAT_03', 'PCIAT-PCIAT_05', 'PCIAT-PCIAT_15', 'PCIAT-PCIAT_17', 'PCIAT-PCIAT_Total']\n"
     ]
    }
   ],
   "source": [
    "importances = model.feature_importances_\n",
    "feature_names = new_X_train.columns\n",
    "important_features = [name for name, importance in zip(feature_names, importances) if importance > 0.05]\n",
    "print(important_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6b05a5",
   "metadata": {},
   "source": [
    "We can see that the most significative features are the one relative to PCIAT that means Parent-Child Internet Addiction Test. <br>\n",
    "These features measures characteristics and behaviors associated with compulsive use of the Internet. <br>\n",
    "From the description we can understand that they can easily be very useful for the prediction, but unfortunately they are not present in the `test.csv` file, so to have a coherent predictor I've removed those features since in practice, they don't provide any help in the classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "10cad809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82\n",
      "59\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv(\"data/train.csv\")\n",
    "df_test = pd.read_csv(\"data/test.csv\")\n",
    "print(df_train.shape[1])\n",
    "print(df_test.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "67a35a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PCIAT-PCIAT_03', 'PCIAT-PCIAT_08', 'PCIAT-PCIAT_19', 'PCIAT-PCIAT_10', 'PCIAT-PCIAT_13', 'PCIAT-PCIAT_15', 'PCIAT-PCIAT_18', 'PCIAT-Season', 'PCIAT-PCIAT_11', 'PCIAT-PCIAT_14', 'PCIAT-PCIAT_01', 'PCIAT-PCIAT_17', 'PCIAT-PCIAT_02', 'PCIAT-PCIAT_07', 'PCIAT-PCIAT_05', 'PCIAT-PCIAT_16', 'PCIAT-PCIAT_09', 'PCIAT-PCIAT_06', 'PCIAT-PCIAT_04', 'PCIAT-PCIAT_Total', 'PCIAT-PCIAT_20', 'PCIAT-PCIAT_12']\n"
     ]
    }
   ],
   "source": [
    "train_features = df_train.columns.tolist()\n",
    "test_features = df_test.columns.tolist()\n",
    "features_toremove =  list(set(train_features) - set(test_features) - {'sii'})\n",
    "print(features_toremove)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b583047a",
   "metadata": {},
   "source": [
    "As we can see, the features that are not included in the test set are the PCIAT features.<br>\n",
    "Let's now try to process the cleaning without including the PCIAT features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ffe8b8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_train['id']\n",
    "for col in features_toremove: # this time we remove the PCIAT features from the train set\n",
    "    del df_train[col]\n",
    "df_train.dropna(subset=['sii'], inplace=True)\n",
    "#df_train.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fe65e46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this part is the same as before\n",
    "\n",
    "physical_measures_df = pd.read_csv('data/physical_measures.csv')\n",
    "\n",
    "df_train = df_train.merge( physical_measures_df, on=['Basic_Demos-Age', 'Basic_Demos-Sex'], suffixes=('', '_avg'))\n",
    "cols = ['Physical-BMI','Physical-Height','Physical-Weight','Physical-Waist_Circumference','Physical-Diastolic_BP','Physical-HeartRate','Physical-Systolic_BP']\n",
    "tot_nan_phys = df_train[cols].isna().all(axis=1)\n",
    "\n",
    "for col in cols:\n",
    "    df_train.loc[tot_nan_phys, col] = df_train.loc[tot_nan_phys, f\"{col}_avg\"]\n",
    "    del df_train[f\"{col}_avg\"]\n",
    "\n",
    "\n",
    "X = df_train.iloc[:, :-1]\n",
    "y = df_train.iloc[:, -1]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "\n",
    "is_numerical = np.array([np.issubdtype(dtype, np.number) for dtype in X.dtypes])  \n",
    "numerical_idx = np.flatnonzero(is_numerical) \n",
    "new_X_train = X_train.iloc[:, numerical_idx]\n",
    "new_X_test = X_test.iloc[:, numerical_idx]\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "imputer = KNNImputer(n_neighbors=3)\n",
    "\n",
    "scaled_train = scaler.fit_transform(new_X_train)\n",
    "X_array = imputer.fit_transform(scaled_train)\n",
    "X_array = scaler.inverse_transform(X_array)\n",
    "new_X_train = pd.DataFrame(X_array, columns=new_X_train.columns, index=new_X_train.index) # convert into a dataframe since X_array is of type ndarray\n",
    "\n",
    "scaled_test = scaler.fit_transform(new_X_test)\n",
    "X_array = imputer.fit_transform(scaled_test)\n",
    "X_array = scaler.inverse_transform(X_array)\n",
    "new_X_test = pd.DataFrame(X_array, columns=new_X_test.columns, index=new_X_test.index)\n",
    "\n",
    "categorical_idx = np.flatnonzero(is_numerical==False)\n",
    "categorical_X_train = X_train.iloc[:, categorical_idx]\n",
    "categorical_X_test = X_test.iloc[:, categorical_idx]\n",
    "\n",
    "imputer = SimpleImputer(strategy='most_frequent')\n",
    "X_array = imputer.fit_transform(categorical_X_train)\n",
    "categorical_X_train = pd.DataFrame(X_array, columns=categorical_X_train.columns, index=categorical_X_train.index)\n",
    "\n",
    "X_array = imputer.fit_transform(categorical_X_test)\n",
    "categorical_X_test = pd.DataFrame(X_array, columns=categorical_X_test.columns, index=categorical_X_test.index)\n",
    "\n",
    "\n",
    "oh = OneHotEncoder(sparse_output=False)\n",
    "\n",
    "oh.fit(categorical_X_train)\n",
    "encoded = oh.transform(categorical_X_train)\n",
    "\n",
    "for i, col in enumerate(oh.get_feature_names_out()):\n",
    "    new_X_train = new_X_train.copy()\n",
    "    new_X_train[col] = encoded[:, i]\n",
    "\n",
    "oh.fit(categorical_X_test)\n",
    "encoded = oh.transform(categorical_X_test)\n",
    "\n",
    "for i, col in enumerate(oh.get_feature_names_out()):\n",
    "    new_X_test = new_X_test.copy()\n",
    "    new_X_test[col] = encoded[:, i]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d533c460",
   "metadata": {},
   "source": [
    "Let's see now how our Random Forest Classifier behave:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fc798bea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority class accuracy: 0.558\n",
      "Test Accuracy: 0.584\n"
     ]
    }
   ],
   "source": [
    "baseline_accuracy = y_test.value_counts().max() / y_test.value_counts().sum()\n",
    "print (f\"Majority class accuracy: {baseline_accuracy:.3f}\")\n",
    "model = RandomForestClassifier()\n",
    "model.fit(new_X_train, y_train)\n",
    "\n",
    "#print (\"Best Score: {:.3f}\".format(model.score(new_X_train, y_train)) )\n",
    "\n",
    "test_acc = accuracy_score(y_true = y_test, y_pred = model.predict(new_X_test) )\n",
    "print (\"Test Accuracy: {:.3f}\".format(test_acc) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff1a016",
   "metadata": {},
   "source": [
    "This time that we didn't take into consideration the PCIAT features, we get a classifier that is very similiar to the naive classifier, so we are not really satisfied by it.<br>\n",
    "In the next notebook we will try to get a better Random Forest classifier."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
